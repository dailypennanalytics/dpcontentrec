{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DP ANALYTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
      "\u001b[K     |████████████████████████████████| 378kB 5.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py in /Users/davidfan/anaconda3/lib/python3.7/site-packages (from keras) (2.9.0)\n",
      "Requirement already satisfied: pyyaml in /Users/davidfan/anaconda3/lib/python3.7/site-packages (from keras) (5.1.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/davidfan/anaconda3/lib/python3.7/site-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /Users/davidfan/anaconda3/lib/python3.7/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /Users/davidfan/anaconda3/lib/python3.7/site-packages (from keras) (1.16.4)\n",
      "Requirement already satisfied: scipy>=0.14 in /Users/davidfan/anaconda3/lib/python3.7/site-packages (from keras) (1.3.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /Users/davidfan/anaconda3/lib/python3.7/site-packages (from keras) (1.0.8)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.3.1\n"
     ]
    }
   ],
   "source": [
    "! pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "import gensim\n",
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# for doc in np.arange(0,len(processed_desc)):\n",
    "#     processed_desc[doc] = processed_desc[doc] + [\"_\".join(w) for w in ngrams(processed_desc[doc], 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean the articles\n",
    "stopword = set(stopwords.words('english'))\n",
    "stopword.add(\"'s\")\n",
    "stemmer = PorterStemmer()\n",
    "def is_word(x):\n",
    "    return len(re.findall('[0-9' + string.punctuation +']|nbsp', x)) == 0\n",
    "def clean_articles(syllabus):\n",
    "    # tokenizes it using nltk.word_tokenize\n",
    "    result = word_tokenize(syllabus)\n",
    "    \n",
    "    # converts it to lower case\n",
    "    result = [token.lower() for token in result]\n",
    "    \n",
    "    # removes stopwords that are present in the set from the cell above\n",
    "    result = [token for token in result if not token in stopword]\n",
    "    \n",
    "    # uses stemmer to cut the word down to its stem\n",
    "    result = [stemmer.stem(token) for token in result]\n",
    "    \n",
    "    # uses has_letter to remove words that don't have any letters\n",
    "    result = [token for token in result if is_word(token) == True]\n",
    "    \n",
    "    # keeps only those words with length greater than 1 and less than 20.\n",
    "    result = [token for token in result if ((len(token) > 2) and (len(token) < 20))]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Corpus Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "admin = 'mysql://admin:Da!lyP3nn12!@dp-article-content.capz6asc9wzb.us-east-2.rds.amazonaws.com:3306/dpcontent'\n",
    "user = 'mysql://dpdata:hootsuite130!@dp-article-content.capz6asc9wzb.us-east-2.rds.amazonaws.com:3306/dpcontent'\n",
    "engine = db.create_engine(admin)\n",
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNT(*)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   COUNT(*)\n",
       "0     84713"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descrip = pd.read_sql_query(\"SELECT COUNT(*) FROM content\", connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dpstaff/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3057: DtypeWarning: Columns (0,1,2,3,4,5,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#quick data fixes\n",
    "train_data = descrip\n",
    "train_data = train_data.dropna(subset = [\"content\"]).reset_index().drop([\"index\",descrip.columns[0]],axis = 1)\n",
    "train_data = train_data.drop_duplicates(\"title_url\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop Through to Clean Articles\n",
    "desc_list = {}\n",
    "length = len(train_data)\n",
    "counter = 0\n",
    "processed_desc = []\n",
    "\n",
    "for i in range(length):\n",
    "    try:\n",
    "        desc_list[i] = clean_articles(train_data['content'][i])\n",
    "    except:\n",
    "        print(i)\n",
    "\n",
    "for i in desc_list:\n",
    "    processed_desc.append(desc_list.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_desc)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_desc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA Models\n",
    "lda_model_desc = gensim.models.ldamodel.LdaModel(corpus=bow_corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=35, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=3000,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for idx, topic in lda_model_desc.print_topics(-1):\n",
    "#     print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "# print('\\nPerplexity: ', lda_model_desc.log_perplexity(bow_corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# # Compute Coherence Score\n",
    "# coherence_model_lda = CoherenceModel(model=lda_model_desc, texts=processed_desc, dictionary=dictionary, coherence='c_v')\n",
    "# coherence_lda = coherence_model_lda.get_coherence()\n",
    "# print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=num_topics, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=3000,\n",
    "                                           passes=3,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can take a long time to run.\n",
    "# model_list, coherence_values = compute_coherence_values(dictionary=dictionary, \n",
    "#                                                         corpus=bow_corpus, texts=processed_desc, \n",
    "#                                                         start=35, limit=50, step=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "# for m, cv in zip(x, coherence_values):\n",
    "#     print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model and print the topics\n",
    "#optimal_model = model_list[0]\n",
    "optimal_model = lda_model_desc\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "Topic_list = []\n",
    "for idx, topic in optimal_model.print_topics(-1):\n",
    "    Topic_list.append('Topic: {} Words: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_probs(x):\n",
    "    return(optimal_model[x][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_frame = pd.DataFrame(columns = [\"title\", \"content\"]+list(np.arange(0,35)))\n",
    "def output_probs(x):\n",
    "    vector = optimal_model[x][0]\n",
    "    base = pd.DataFrame(np.array(vector).T)\n",
    "    base.columns = base.iloc[0,:]\n",
    "    base = base.drop([0]).astype(float)\n",
    "    return(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "base_frame = pd.concat((list(map(output_probs,bow_corpus))),ignore_index=True).fillna(0)\n",
    "end = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing\n",
    "\n",
    "# try:\n",
    "#     cpus = multiprocessing.cpu_count()\n",
    "# except NotImplementedError:\n",
    "#     cpus = 2   # arbitrary default\n",
    "\n",
    "# pool = multiprocessing.Pool(processes=cpus)\n",
    "# try_bf = pd.concat((list(pool.map(output_probs, bow_corpus))),ignore_index=True).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_title = []\n",
    "for i in np.arange(1,36):\n",
    "    content_title.append(\"Content\"+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_frame.columns = content_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_frame['title_url'] = train_data.title_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list = {}\n",
    "length = len(train_data)\n",
    "\n",
    "for i in range(length):\n",
    "    try:\n",
    "        title_list[i] = clean_articles(train_data['title'][i])\n",
    "    except:\n",
    "        title_list[i] = clean_articles(\" \".join(train_data['slug'][i].split(\"-\")))\n",
    "\n",
    "processed_title = []\n",
    "for i in title_list:\n",
    "    processed_title.append(title_list.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_title = gensim.corpora.Dictionary(processed_title)\n",
    "bow_corpus_title = [dictionary_title.doc2bow(doc) for doc in processed_title]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_title = gensim.models.ldamodel.LdaModel(corpus=bow_corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=45, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=3000,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can take a long time to run.\n",
    "# t_model_list, t_coherence_values = compute_coherence_values(dictionary=dictionary_title, \n",
    "#                                                         corpus=bow_corpus_title, texts=processed_title, \n",
    "#                                                         start=40, limit=60, step=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "# for m, cv in zip(x, t_coherence_values):\n",
    "#     print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model and print the topics\n",
    "t_optimal_model = lda_model_title\n",
    "t_model_topics = t_optimal_model.show_topics(formatted=False)\n",
    "T_Topic_list = []\n",
    "for idx, topic in t_optimal_model.print_topics(-1):\n",
    "    T_Topic_list.append('Topic: {} Words: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_frame_t = pd.DataFrame(columns = [\"title\", \"content\"]+list(np.arange(0,46)))\n",
    "def t_output_probs(x):\n",
    "    vector = t_optimal_model[x][0]\n",
    "    base = pd.DataFrame(np.array(vector).T)\n",
    "    base.columns = base.iloc[0,:]\n",
    "    base = base.drop([0]).astype(float)\n",
    "    return(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_frame_t = pd.concat((list(map(t_output_probs,bow_corpus_title))),ignore_index=True).fillna(0)\n",
    "base_frame_t = base_frame_t/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_title = []\n",
    "for i in np.arange(1,46):\n",
    "    title_title.append(\"Title\"+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_frame_t.columns = title_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_frame_t['title_url'] = train_data.title_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date Time Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = base_frame.merge(base_frame_t,on = [\"title_url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comp = comp.drop(['content','title'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp['published_time'] = descrip.reset_index()['published_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def date_transform(x):\n",
    "    if x is None or x == \"nan\":\n",
    "        return 0\n",
    "    return datetime.timestamp(datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_date = np.vectorize(date_transform)\n",
    "comp.published_time = comp.published_time.astype(str)\n",
    "comp[\"publish_time_num\"] = vect_date(comp.published_time)\n",
    "usable_comp = comp.drop(['published_time'],axis=1)\n",
    "\n",
    "usable_comp['publish_time_num'] = usable_comp['publish_time_num']/1000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_comp = usable_comp\n",
    "usable_comp = usable_comp.drop('publish_time_num', axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Based Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = usable_comp.title_url\n",
    "usable_comp = usable_comp.drop(['title_url'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_recommendation(content,url_list,url,num_of_rec):\n",
    "    months = 5\n",
    "    document_matrix = np.array(content)\n",
    "    AOI = np.array(content[url_list == url])\n",
    "    rec_list = np.argsort(\n",
    "        np.sum(\n",
    "            np.abs(\n",
    "                document_matrix - AOI\n",
    "                )\n",
    "            ,axis = 1)\n",
    "    )\n",
    "    filtered_list = rec_list[abs(extract_date_vect(url_list)[rec_list] - extract_date(url)) <= months]\n",
    "    return(filtered_list[1:num_of_rec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_sampling(url_list,num_list,num_of_sample):\n",
    "    return(url_list[np.random.choice(num_list,num_of_sample,replace = False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_ranking(content,url_list,num_list,num_of_sample):\n",
    "    content = np.array(content)\n",
    "    time = content[num_list,content.shape[1]-1]\n",
    "    return(url_list[num_list[np.argsort(time)][::-1][0:num_of_sample]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "#give_recommendation(usable_comp,slug_list,slug_list[0],15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slug_list[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "#article_sampling(slug_list,give_recommendation(usable_comp,slug_list,slug_list[4],15),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data.iloc[1162,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usable_comp['key'] =1\n",
    "# cross = usable_comp.merge(usable_comp, on = 'key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full = cross[cross['Slug_x'] != cross['Slug_y']]\n",
    "# full.loc[:,'distance'] = 0\n",
    "# x = np.array(full.loc[:,[(\"_x\" in i and \"Slug\" not in i and \"Title\" not in i) for i in full.columns]])\n",
    "# y = np.array(full.loc[:,[(\"_y\" in i and \"Slug\" not in i and \"Title\" not in i) for i in full.columns]])\n",
    "# full.loc[:,'distance'] = np.sum((x-y)**2, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simp = full.iloc[:,[1,93,183]]\n",
    "# spread = simp.pivot(index = \"Slug_x\", columns = 'Slug_y',values = 'distance')\n",
    "# y_slug = spread.columns\n",
    "# x_slug = spread.index\n",
    "# spread_matrix = np.array(spread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# x_slug[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# New Article Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_data = pd.read_csv(\"raw_data/newcontent.csv\")\n",
    "# #new_data = new_data[[i in set(new_data.title_url) - set(train_data.title_url) \n",
    "#                      #for i in new_data.title_url]].reset_index()\n",
    "# #new_data = new_data.iloc[:,2:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title_new_list = {}\n",
    "# length = len(new_data)\n",
    "\n",
    "# for i in range(length):\n",
    "#     try:\n",
    "#         title_new_list[i] = clean_articles(new_data['title'][i])\n",
    "#     except:\n",
    "#         title_new_list[i] = clean_articles(\" \".join(new_data['slug'][i].split(\"-\")))\n",
    "\n",
    "# processed_new_title = []\n",
    "# for i in title_new_list:\n",
    "#     processed_new_title.append(title_new_list.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desc_new_list = {}\n",
    "# length = len(new_data)\n",
    "# counter = 0\n",
    "# processed_new_desc = []\n",
    "\n",
    "# for i in range(length):\n",
    "#     try:\n",
    "#         desc_new_list[i] = clean_articles(new_data['content'][i])\n",
    "#     except:\n",
    "#         print(i)\n",
    "\n",
    "# for i in desc_new_list:\n",
    "#     processed_new_desc.append(desc_new_list.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bow_corpus_new_title = [dictionary_title.doc2bow(doc) for doc in processed_new_title]\n",
    "# bow_corpus_new = [dictionary.doc2bow(doc) for doc in processed_new_desc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_title_df = pd.concat((list(map(t_output_probs,bow_corpus_new_title))),ignore_index=True).fillna(0)\n",
    "# new_content_df =  pd.concat((list(map(output_probs,bow_corpus_new))),ignore_index=True).fillna(0)\n",
    "# new_title_df.columns = title_title\n",
    "# new_content_df.columns = content_title\n",
    "# new_title_df[\"title_url\"] = new_data.title_url\n",
    "# new_content_df[\"title_url\"] = new_data.title_url\n",
    "\n",
    "# new_comp = new_content_df.merge(new_title_df,on = [\"title_url\"])\n",
    "# new_comp['published_time'] = new_data['published_date']\n",
    "\n",
    "# new_comp.published_time = new_comp.published_time.astype(str)\n",
    "# new_comp[\"publish_time_num\"] = vect_date(new_comp.published_time)\n",
    "# new_comp = new_comp.drop(['published_time'],axis=1)\n",
    "# new_comp['publish_time_num'] = new_comp['publish_time_num']/1000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# added_comp = full_comp.append(new_comp,ignore_index= True,sort =  False)\n",
    "# added_url_list = added_comp.title_url\n",
    "# added_comp = added_comp.drop(['title_url'],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_date(x):\n",
    "#     try:\n",
    "#         val = int(x[0:4])*12 + int(x[5:7])\n",
    "#     except:\n",
    "#         val = 0\n",
    "#     return(val)\n",
    "# extract_date_vect = np.vectorize(extract_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def give_recommendation(content,url_list,url,num_of_rec):\n",
    "#     months = 5\n",
    "#     document_matrix = np.array(content)\n",
    "#     AOI = np.array(content[url_list == url])\n",
    "#     rec_list = np.argsort(\n",
    "#         np.sum(\n",
    "#             np.abs(\n",
    "#                 document_matrix - AOI\n",
    "#                 )\n",
    "#             ,axis = 1)\n",
    "#     )\n",
    "#     filtered_list = rec_list[abs(extract_date_vect(url_list)[rec_list] - extract_date(url)) <= months]\n",
    "#     return(filtered_list[1:num_of_rec])\n",
    "\n",
    "# def article_sampling(url_list,num_list,num_of_sample):\n",
    "#     return(url_list[np.random.choice(num_list,num_of_sample,replace = False)])\n",
    "\n",
    "# def article_ranking(content,url_list,num_list,num_of_sample):\n",
    "#     content = np.array(content)\n",
    "#     time = content[num_list,content.shape[1]-1]\n",
    "#     return(url_list[num_list[np.argsort(time)][::-1][0:num_of_sample]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added_url_list = np.load(\"Numpy_Files/title.npy\",allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added_comp = np.load(\"Numpy_Files/content.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added_url_list[3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give_recommendation(added_comp,added_url_list,added_url_list[40],6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article_sampling(added_url_list,give_recommendation(added_comp,added_url_list,added_url_list[3000],6),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article_ranking(added_comp,added_url_list,give_recommendation(added_comp,added_url_list,added_url_list[40],6),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added_url_list[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article_sampling(added_url_list,give_recommendation(added_comp,added_url_list,added_url_list[14],6),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article_ranking(np.array(added_comp),np.array(added_url_list),give_recommendation(added_comp,added_url_list,added_url_list[12],10),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Updating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.040*\"food\" + 0.031*\"dine\" + 0.023*\"restaur\" + 0.018*\"eat\" + 0.017*\"meal\" + 0.012*\"bar\" + 0.011*\"dog\" + 0.011*\"drink\" + 0.010*\"hall\" + 0.009*\"ice\"'"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_model.print_topic(0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimal_model.update(bow_corpus_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.040*\"food\" + 0.031*\"dine\" + 0.023*\"restaur\" + 0.018*\"eat\" + 0.017*\"meal\" + 0.012*\"bar\" + 0.011*\"dog\" + 0.011*\"drink\" + 0.010*\"hall\" + 0.009*\"ice\"'"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_model.print_topic(0,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Numpy_Files/content\",np.array(added_comp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Numpy_Files/title\",np.array(added_url_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_model.save(\"LDA_Content_Models/content_model.lda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_optimal_model.save(\"LDA_Title_Model/title_model.lda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_title.save(\"Dictionary_Files/title_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.save(\"Dictionary_Files/content_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final frame has content before title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "#added_url_list = np.load(\"Numpy_Files/title.npy\",allow_pickle=True)\n",
    "added_url_list = np.array(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "#added_comp = np.load(\"Numpy_Files/content.npy\")\n",
    "added_comp = np.array(usable_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean the articles\n",
    "stopword = set(stopwords.words('english'))\n",
    "stopword.add(\"'s\")\n",
    "stemmer = PorterStemmer()\n",
    "def is_word(x):\n",
    "    return len(re.findall('[0-9' + string.punctuation +']|nbsp', x)) == 0\n",
    "def clean_articles(syllabus):\n",
    "    # tokenizes it using nltk.word_tokenize\n",
    "    result = word_tokenize(syllabus)\n",
    "    \n",
    "    # converts it to lower case\n",
    "    result = [token.lower() for token in result]\n",
    "    \n",
    "    # removes stopwords that are present in the set from the cell above\n",
    "    result = [token for token in result if not token in stopword]\n",
    "    \n",
    "    # uses stemmer to cut the word down to its stem\n",
    "    result = [stemmer.stem(token) for token in result]\n",
    "    \n",
    "    # uses has_letter to remove words that don't have any letters\n",
    "    result = [token for token in result if is_word(token) == True]\n",
    "    \n",
    "    # keeps only those words with length greater than 1 and less than 20.\n",
    "    result = [token for token in result if ((len(token) > 2) and (len(token) < 20))]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def date_transform(x):\n",
    "    if x is None or x == \"nan\":\n",
    "        return 0\n",
    "    return datetime.timestamp(datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try1 = gensim.models.LdaModel.load(\"LDA_Title_Model/title_model.lda\")\n",
    "try1 = lda_model_title\n",
    "#try2 = gensim.models.LdaModel.load(\"LDA_Content_Models/content_model.lda\")\n",
    "try2 = lda_model_desc\n",
    "\n",
    "dictionary_title = gensim.corpora.Dictionary.load(\"Dictionary_Files/title_dict\")\n",
    "dictionary = gensim.corpora.Dictionary.load(\"Dictionary_Files/content_dict\")\n",
    "\n",
    "def data_cleaning(new_data):\n",
    "    try:\n",
    "        title = clean_articles(new_data['title'])\n",
    "    except:\n",
    "        title = clean_articles(\" \".join(new_data['slug'].split(\"-\")))\n",
    "    try:\n",
    "        desc = clean_articles(new_data['content'])\n",
    "    except:\n",
    "        print(new_data['title_url'])\n",
    "    bow_title = dictionary_title.doc2bow(title)\n",
    "    bow_content = dictionary.doc2bow(desc)\n",
    "    dummy_frame_t = pd.DataFrame(columns = list(np.arange(0,45)))\n",
    "    title_probs = pd.concat([dummy_frame_t,t_output_probs(try1,bow_title)]).fillna(0)\n",
    "    dummy_frame = pd.DataFrame(columns = list(np.arange(0,35)))\n",
    "    content_probs = pd.concat([dummy_frame,output_probs(try2,bow_content)]).fillna(0)\n",
    "    return([new_data.title_url],np.concatenate((content_probs,title_probs),\n",
    "                         axis = None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_probs(optimal_model,x):\n",
    "    vector = optimal_model[x][0]\n",
    "    base = pd.DataFrame(np.array(vector).T)\n",
    "    base.columns = base.iloc[0,:]\n",
    "    base = base.drop([0]).astype(float)\n",
    "    return(base)\n",
    "\n",
    "def t_output_probs(t_optimal_model,x):\n",
    "    vector = t_optimal_model[x][0]\n",
    "    base = pd.DataFrame(np.array(vector).T)\n",
    "    base.columns = base.iloc[0,:]\n",
    "    base = base.drop([0]).astype(float)\n",
    "    return(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date(x):\n",
    "    try:\n",
    "        val = int(x[0:4])*12 + int(x[5:7])\n",
    "    except:\n",
    "        val = 0\n",
    "    return(val)\n",
    "extract_date_vect = np.vectorize(extract_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_recommendation(content,url_list,url,num_of_rec):\n",
    "    months = 100\n",
    "    document_matrix = np.array(content)\n",
    "    AOI = np.array(content[url_list == url])\n",
    "    rec_list = np.argsort(\n",
    "        np.sum(\n",
    "            np.abs(\n",
    "                document_matrix - AOI\n",
    "                )\n",
    "            ,axis = 1)\n",
    "    )\n",
    "    filtered_list = rec_list[abs(extract_date_vect(url_list)[rec_list] - extract_date(url)) <= months]\n",
    "    return(filtered_list[1:num_of_rec])\n",
    "\n",
    "def article_sampling(url_list,num_list,num_of_sample):\n",
    "    return(url_list[np.random.choice(num_list,num_of_sample,replace = False)])\n",
    "\n",
    "def article_ranking(content,url_list,num_list,num_of_sample):\n",
    "    content = np.array(content)\n",
    "    time = content[num_list,content.shape[1]-1]\n",
    "    return(url_list[num_list[np.argsort(time)][::-1][0:num_of_sample]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date(x):\n",
    "    try:\n",
    "        val = int(x[0:4])*12 + int(x[5:7])\n",
    "    except:\n",
    "        val = 0\n",
    "    return(val)\n",
    "extract_date_vect = np.vectorize(extract_date)\n",
    "\n",
    "def give_recommendation(content,url_list,url,num_of_rec,months):\n",
    "    document_matrix = np.array(content)\n",
    "    AOI = np.array(content[url_list == url])\n",
    "    rec_list = np.argsort(\n",
    "        np.sum(\n",
    "            np.abs(\n",
    "                document_matrix - AOI\n",
    "                )\n",
    "            ,axis = 1)\n",
    "    )\n",
    "    filtered_list = rec_list[abs(extract_date_vect(url_list)[rec_list] - extract_date(url)) <= months]\n",
    "    return(filtered_list[1:num_of_rec])\n",
    "\n",
    "def article_sampling(url_list,num_list,num_of_sample):\n",
    "    return(url_list[np.random.choice(num_list,num_of_sample,replace = False)])\n",
    "\n",
    "def article_ranking(content,url_list,num_list,num_of_sample):\n",
    "    content = np.array(content)\n",
    "    time = content[num_list,content.shape[1]-1]\n",
    "    return(url_list[num_list[np.argsort(time)][::-1][0:num_of_sample]])# new_data = pd.read_csv(\"raw_data/newcontent.csv\")\n",
    "# new_data = new_data[[i in set(new_data.title_url) - set(train_data.title_url) \n",
    "#                      for i in new_data.title_url]].reset_index()\n",
    "# new_data = new_data.iloc[:,2:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full New Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiofiles\n",
    "import aiohttp\n",
    "import jwt\n",
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "## pk = public_key\n",
    "## secret = private_key\n",
    "## jwt.encode({'pk': 'payload'}, 'secret', algorithm='HS256')\n",
    "encoded_jwt = jwt.encode({'pk': 'pk_KoukiYYjUNBNbkXfOv7Lk7gkfNBo5g'}, 'sk_LYtda2Eini8LCUr7yOGbeG0Pb2Geid', algorithm='HS256')\n",
    "                          \n",
    "\n",
    "jwt = encoded_jwt.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_new(slug):\n",
    "    #Make Request\n",
    "    endpoint = 'https://dpn.ceo.getsnworks.com/v3/search?type=content&keywords=' + slug\n",
    "    headers = {\"Authorization\": \"Bearer \" + jwt}\n",
    "    r = requests.get(url = endpoint,headers = headers)\n",
    "    item = r.json()['items'][0]\n",
    "    \n",
    "    #Empty Frame\n",
    "    uuid = []\n",
    "    ids = []\n",
    "    titles = []\n",
    "    content = []\n",
    "    slugs = []\n",
    "    published_dates = []\n",
    "    abstract = []\n",
    "    title_url = []\n",
    "    \n",
    "    #Append Frame\n",
    "    uuid.append(item['uuid'])\n",
    "    ids.append(item['id'])\n",
    "    titles.append(item['title'])\n",
    "    title_url.append(item['published_at'][0:4] + \"/\" + item['published_at'][5:7] + \"/\" + item['title_url'])\n",
    "    slugs.append(item['slug'])\n",
    "    soup = BeautifulSoup(item['content'])\n",
    "    for script in soup(['script','style']):\n",
    "        script.decompose()\n",
    "    content.append(soup.get_text().replace(u'\\xa0',u' ').replace(\"\\n\",\" \"))\n",
    "    published_dates.append(item['published_at'])\n",
    "    abstract.append(item['abstract'])  \n",
    "    \n",
    "    #Assemble Frame\n",
    "    articles_df = pd.DataFrame(data={'id' : ids,'uuid' : uuid,'title': titles, 'slug': slugs, 'content': content, \n",
    "                                 'published_date': published_dates,\"title_url\":title_url})\n",
    "    \n",
    "    return(articles_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = fetch_new('thanksgiving-penn-dining-halls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['2019/11/thanksgiving-penn-dining-halls'],\n",
       " array([0.36422294, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.06467278, 0.08426073, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.06906243, 0.        , 0.        , 0.27159113,\n",
       "        0.09742931, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.03964834, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.01438516, 0.01100621,\n",
       "        0.        , 0.01148206, 0.        , 0.03714138, 0.        ,\n",
       "        0.01906855, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.24263237,\n",
       "        0.        , 0.        , 0.01454959, 0.01044287, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.01031506, 0.01140327, 0.        , 0.11145388,\n",
       "        0.        , 0.11609112, 0.01131441, 0.        , 0.        ,\n",
       "        0.03860552, 0.14774451, 0.        , 0.        , 0.        ]))"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaning(new_data.iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_new(old_content,new_content,old_title,new_title):\n",
    "    full_content = np.vstack([old_content, new_content])\n",
    "    full_title = np.array(list(old_title) + list(new_title))\n",
    "    #np.save(\"content\",full_content)\n",
    "    #np.save(\"title\",full_title)\n",
    "    return full_content,full_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FULL PIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2019/10/penn-admissions-harvard-varsity-blues-jerome-allen-scandal-athletics-recruiting',\n",
       "       '2019/10/dia-de-los-muertos-penn-museum-photo-essay',\n",
       "       '2019/10/is-stat-so-week-of-october-27', ...,\n",
       "       '1970/01/penn-abroad-deals-with-emergencies-overseas',\n",
       "       '1970/01/new-abroad-director',\n",
       "       '1970/01/study-abroad-by-the-numbers'], dtype=object)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "added_url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_recommender(article_title_url,added_url_list,added_comp):\n",
    "    if np.any(added_url_list == article_title_url):\n",
    "        return(added_comp,added_url_list,\n",
    "               article_sampling(added_url_list,give_recommendation(added_comp,added_url_list,article_title_url,\n",
    "                                                                   10,12),3))\n",
    "    else:\n",
    "        new_data = fetch_new(article_title_url[8:])\n",
    "        new_title,new_content = data_cleaning(new_data.iloc[0,:])\n",
    "        comp,url_list = write_new(added_comp, new_content, added_url_list, new_title)\n",
    "        return(comp,url_list,\n",
    "               article_sampling(url_list,give_recommendation(comp,url_list,article_title_url,\n",
    "                                                             10,12),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2019/03/admissions-scandal-final-four-march-madness-ivy-league-athletics-upenn-philadelphia',\n",
       "       '2019/03/admissions-bribery-jerome-allen-basketball-ivy-league-upenn-philadelphia',\n",
       "       '2019/09/penn-college-admissions-harvard-yale-donations-lauder'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_recommender('2019/10/penn-admissions-harvard-varsity-blues-jerome-allen-scandal-athletics-recruiting',\n",
    "                   added_url_list,\n",
    "                   added_comp)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2019/03/upenn-admissions-bribery-eric-furda-jerome-allen-college',\n",
       "       '2018/11/harvard-affirmative-action-trial-legacy-student-penn-philly',\n",
       "       '2019/03/admissions-jerome-allen-ivy-league-fact-checking-upenn-philadelphia'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_recommender('2019/10/penn-admissions-harvard-varsity-blues-jerome-allen-scandal-athletics-recruiting',\n",
    "                   added_url_list,\n",
    "                   added_comp)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2019/02/penn-athletics-black-history-month-feature-part-ii-childs-davis-nwokedi',\n",
       "       '2019/01/penn-athletics-senior-associate-ad-david-leach-stepped-down-calhoun',\n",
       "       '2019/01/penn-mens-basketball-1979-final-four-team-vincent-ross-david-jackson-alumni'],\n",
       "      dtype='<U263')"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_recommender('2019/11/penn-athletics-ncaa-announcement-paying-student-athletes-amateur-status',\n",
    "                   added_url_list,\n",
    "                   added_comp)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2018/11/history-student-society-penn-upenn-philadelphia',\n",
       "       '2019/07/khan-academy-resources-education-motivation-technology-alex-silberzweig-ivy-league-upenn-philadelphia',\n",
       "       '2019/03/women-of-color-academia-penn-professor-wage-gap'],\n",
       "      dtype='<U263')"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_recommender('2019/10/architecture-topographical-stories-lived-experience-class-upenn',\n",
    "                   added_url_list,\n",
    "                   added_comp)[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
